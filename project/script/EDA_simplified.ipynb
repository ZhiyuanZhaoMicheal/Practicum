{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# EDA - Simplified Version\n",
    "\n",
    "## Key Features:\n",
    "1. `event_key` parsed directly from filename\n",
    "2. Snow pixels are **included** in images, but tracked via `snow_frac`\n",
    "3. Can filter by snow fraction threshold in analysis\n",
    "4. Useful for winter storm events (e.g., 2021 Texas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, box\n",
    "from rasterio.mask import mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec0",
   "metadata": {},
   "source": [
    "# 0) Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path(\"..\")  # project/\n",
    "DATA = BASE / \"data\"\n",
    "RAW_IMGS = DATA / \"raw\" / \"imgs\"\n",
    "PROCESSED = DATA / \"processed\"\n",
    "RESULT = BASE / \"result\"\n",
    "RESULT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PLOT_DIR = RESULT / \"plots\"\n",
    "PLOT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# New v2 folders\n",
    "IMG_DIRS = [\n",
    "    RAW_IMGS / \"GEE_VNP46A2_outages_v2_Dallas\",\n",
    "    RAW_IMGS / \"GEE_VNP46A2_outages_v2_Harris\",\n",
    "]\n",
    "\n",
    "CLOUD_SUMMARY = RAW_IMGS / \"GEE_VNP46A2_outages_tables\" / \"cloud_fraction_summary_v2.csv\"\n",
    "\n",
    "# Parameters\n",
    "PRE_DAYS = 5\n",
    "POST_DAYS = 5\n",
    "CLOUD_THRESHOLD = 0.3\n",
    "\n",
    "# NEW: Snow filtering threshold\n",
    "# Set to None to include all images regardless of snow\n",
    "# Set to a value (e.g., 0.5) to exclude images with snow_frac > threshold\n",
    "SNOW_THRESHOLD = None  # None = keep all, 0.5 = exclude if >50% snow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec1",
   "metadata": {},
   "source": [
    "# 1) Read cloud summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud = pd.read_csv(CLOUD_SUMMARY, parse_dates=[\"date\"])\n",
    "cloud[\"date\"] = pd.to_datetime(cloud[\"date\"]).dt.normalize()\n",
    "\n",
    "# Calculate snow fraction\n",
    "cloud[\"snow_frac\"] = cloud[\"snow_px\"] / cloud[\"base_valid_px\"].clip(lower=1)\n",
    "\n",
    "# Mark usable images (cloud filter)\n",
    "cloud[\"usable\"] = (\n",
    "    (cloud[\"img_exists\"] == True) &\n",
    "    (cloud[\"cloud_frac\"] >= 0) &\n",
    "    (cloud[\"cloud_frac\"] <= CLOUD_THRESHOLD) &\n",
    "    (cloud[\"base_valid_px\"] > 0)\n",
    ")\n",
    "\n",
    "# Optional: Apply snow filter\n",
    "if SNOW_THRESHOLD is not None:\n",
    "    cloud[\"usable\"] = cloud[\"usable\"] & (cloud[\"snow_frac\"] <= SNOW_THRESHOLD)\n",
    "    print(f\"Snow threshold applied: <= {SNOW_THRESHOLD}\")\n",
    "else:\n",
    "    print(\"Snow threshold: None (all snow levels included)\")\n",
    "\n",
    "print(f\"\\nCloud summary records: {len(cloud)}\")\n",
    "print(f\"Usable records: {cloud['usable'].sum()}\")\n",
    "\n",
    "# Show snow statistics\n",
    "print(f\"\\nSnow fraction statistics:\")\n",
    "print(cloud[\"snow_frac\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec2",
   "metadata": {},
   "source": [
    "# 2) Parse outage dates from event_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_event_key(event_key):\n",
    "    \"\"\"\n",
    "    Parse event_key to extract county and outage dates\n",
    "    Format: {county}_{startYYYYMMDDHHMM}_{endYYYYMMDDHHMM}\n",
    "    \"\"\"\n",
    "    parts = event_key.split(\"_\")\n",
    "    county = parts[0]\n",
    "    start_str = parts[1]\n",
    "    end_str = parts[2]\n",
    "    \n",
    "    outage_start = pd.to_datetime(start_str, format=\"%Y%m%d%H%M\").normalize()\n",
    "    outage_end = pd.to_datetime(end_str, format=\"%Y%m%d%H%M\").normalize()\n",
    "    \n",
    "    return county, outage_start, outage_end\n",
    "\n",
    "# Apply parsing\n",
    "parsed = cloud[\"event_key\"].apply(parse_event_key)\n",
    "cloud[\"county_parsed\"] = parsed.apply(lambda x: x[0])\n",
    "cloud[\"outage_start\"] = parsed.apply(lambda x: x[1])\n",
    "cloud[\"outage_end\"] = parsed.apply(lambda x: x[2])\n",
    "\n",
    "print(\"Parsed outage dates from event_key\")\n",
    "display(cloud[[\"event_key\", \"county\", \"outage_start\", \"outage_end\"]].drop_duplicates().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec3",
   "metadata": {},
   "source": [
    "# 3) Scan local images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename pattern: {event_key}_{YYYYMMDD}_VNP46A2_ntl.tif\n",
    "pat = re.compile(\n",
    "    r\"^([A-Za-z]+_\\d{12}_\\d{12})_(\\d{8})_VNP46A2_ntl\\.tif$\", \n",
    "    re.I\n",
    ")\n",
    "\n",
    "img_records = []\n",
    "for d in IMG_DIRS:\n",
    "    if not d.exists():\n",
    "        print(f\"Warning: {d} does not exist\")\n",
    "        continue\n",
    "    for fp in d.glob(\"*.tif\"):\n",
    "        m = pat.match(fp.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        event_key = m.group(1)\n",
    "        date = pd.to_datetime(m.group(2), format=\"%Y%m%d\").normalize()\n",
    "        img_records.append({\n",
    "            \"event_key\": event_key,\n",
    "            \"date\": date,\n",
    "            \"path\": fp\n",
    "        })\n",
    "\n",
    "imgs = pd.DataFrame(img_records)\n",
    "print(f\"Found {len(imgs)} images\")\n",
    "\n",
    "if imgs.empty:\n",
    "    raise FileNotFoundError(\"No matching GeoTIFFs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec4",
   "metadata": {},
   "source": [
    "# 4) Merge with cloud summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_usable = cloud.loc[\n",
    "    cloud[\"usable\"], \n",
    "    [\"event_key\", \"date\", \"county\", \"event_id\", \"outage_start\", \"outage_end\", \n",
    "     \"snow_px\", \"base_valid_px\", \"snow_frac\"]\n",
    "].copy()\n",
    "\n",
    "imgs = imgs.merge(cloud_usable, on=[\"event_key\", \"date\"], how=\"inner\")\n",
    "print(f\"Images after merge: {len(imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec5",
   "metadata": {},
   "source": [
    "# 5) Phase labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_phase(row):\n",
    "    pre_start = row[\"outage_start\"] - pd.Timedelta(days=PRE_DAYS)\n",
    "    pre_end = row[\"outage_start\"] - pd.Timedelta(days=1)\n",
    "    post_start = row[\"outage_end\"] + pd.Timedelta(days=1)\n",
    "    post_end = row[\"outage_end\"] + pd.Timedelta(days=POST_DAYS)\n",
    "    \n",
    "    if pre_start <= row[\"date\"] <= pre_end:\n",
    "        return \"pre\"\n",
    "    elif row[\"outage_start\"] <= row[\"date\"] <= row[\"outage_end\"]:\n",
    "        return \"during\"\n",
    "    elif post_start <= row[\"date\"] <= post_end:\n",
    "        return \"post\"\n",
    "    return None\n",
    "\n",
    "imgs[\"phase\"] = imgs.apply(label_phase, axis=1)\n",
    "imgs = imgs[imgs[\"phase\"].isin([\"pre\", \"during\", \"post\"])].copy()\n",
    "print(f\"Images after phase labeling: {len(imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec6",
   "metadata": {},
   "source": [
    "# 6) Calculate mean NTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mean_ntl",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_ntl(tif_path: Path) -> float:\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        arr = src.read(1).astype(\"float32\")\n",
    "        nodata = src.nodata\n",
    "        if nodata is not None:\n",
    "            arr[arr == nodata] = np.nan\n",
    "        return float(np.nanmean(arr))\n",
    "\n",
    "imgs[\"mean_ntl\"] = imgs[\"path\"].apply(mean_ntl)\n",
    "print(f\"Mean NTL calculated for {len(imgs)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec7",
   "metadata": {},
   "source": [
    "# 7) Event availability summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "avail",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_availability = (\n",
    "    imgs.groupby(\"event_key\")\n",
    "        .agg(\n",
    "            county=(\"county\", \"first\"),\n",
    "            event_id=(\"event_id\", \"first\"),\n",
    "            outage_start=(\"outage_start\", \"first\"),\n",
    "            outage_end=(\"outage_end\", \"first\"),\n",
    "            n_pre=(\"phase\", lambda x: (x == \"pre\").sum()),\n",
    "            n_during=(\"phase\", lambda x: (x == \"during\").sum()),\n",
    "            n_post=(\"phase\", lambda x: (x == \"post\").sum()),\n",
    "            total_images=(\"date\", \"count\"),\n",
    "            avg_snow_frac=(\"snow_frac\", \"mean\"),\n",
    "            max_snow_frac=(\"snow_frac\", \"max\"),\n",
    "        )\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "event_availability.to_csv(RESULT / \"event_availability.csv\", index=False)\n",
    "print(\"Event availability:\")\n",
    "display(event_availability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec8",
   "metadata": {},
   "source": [
    "# 8) Event x Phase statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "event_phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_phase = (\n",
    "    imgs.groupby([\"event_key\", \"phase\"])\n",
    "        .agg(\n",
    "            county=(\"county\", \"first\"),\n",
    "            event_id=(\"event_id\", \"first\"),\n",
    "            min_ntl=(\"mean_ntl\", \"min\"),\n",
    "            mean_ntl=(\"mean_ntl\", \"mean\"),\n",
    "            n_images=(\"mean_ntl\", \"count\"),\n",
    "            phase_start=(\"date\", \"min\"),\n",
    "            phase_end=(\"date\", \"max\"),\n",
    "            outage_start=(\"outage_start\", \"first\"),\n",
    "            outage_end=(\"outage_end\", \"first\"),\n",
    "            avg_snow_frac=(\"snow_frac\", \"mean\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"event_key\", \"phase\"])\n",
    ")\n",
    "\n",
    "event_phase.to_csv(RESULT / \"event_phase_stats.csv\", index=False)\n",
    "print(\"Event x Phase stats:\")\n",
    "display(event_phase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec9",
   "metadata": {},
   "source": [
    "# 9) Relative NTL change analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "change",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Relative NTL Change Analysis (during vs pre)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for evt in event_phase[\"event_key\"].unique():\n",
    "    evt_df = event_phase[event_phase[\"event_key\"] == evt]\n",
    "    \n",
    "    pre = evt_df[evt_df[\"phase\"] == \"pre\"]\n",
    "    during = evt_df[evt_df[\"phase\"] == \"during\"]\n",
    "    \n",
    "    if len(pre) > 0 and len(during) > 0:\n",
    "        pre_ntl = pre[\"mean_ntl\"].values[0]\n",
    "        during_ntl = during[\"mean_ntl\"].values[0]\n",
    "        pre_snow = pre[\"avg_snow_frac\"].values[0]\n",
    "        during_snow = during[\"avg_snow_frac\"].values[0]\n",
    "        \n",
    "        change_pct = ((during_ntl - pre_ntl) / pre_ntl) * 100\n",
    "        \n",
    "        results.append({\n",
    "            \"event_key\": evt,\n",
    "            \"county\": pre[\"county\"].values[0],\n",
    "            \"pre_ntl\": round(pre_ntl, 2),\n",
    "            \"during_ntl\": round(during_ntl, 2),\n",
    "            \"change_pct\": round(change_pct, 1),\n",
    "            \"pre_snow_frac\": round(pre_snow, 3),\n",
    "            \"during_snow_frac\": round(during_snow, 3),\n",
    "            \"n_pre\": pre[\"n_images\"].values[0],\n",
    "            \"n_during\": during[\"n_images\"].values[0],\n",
    "        })\n",
    "\n",
    "change_df = pd.DataFrame(results)\n",
    "print(\"\\nNTL Change:\")\n",
    "display(change_df)\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Events with NTL decrease: {(change_df['change_pct'] < 0).sum()} / {len(change_df)}\")\n",
    "print(f\"  Average change: {change_df['change_pct'].mean():.1f}%\")\n",
    "print(f\"  Events with high snow (>10%): {((change_df['pre_snow_frac'] > 0.1) | (change_df['during_snow_frac'] > 0.1)).sum()}\")\n",
    "\n",
    "change_df.to_csv(RESULT / \"ntl_change_analysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec10",
   "metadata": {},
   "source": [
    "# 10) Snow impact analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "snow_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Snow Impact Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Correlation between snow and NTL\n",
    "corr = imgs[[\"snow_frac\", \"mean_ntl\"]].corr().iloc[0, 1]\n",
    "print(f\"\\nCorrelation (snow_frac vs mean_ntl): {corr:.3f}\")\n",
    "\n",
    "# Snow by phase\n",
    "print(\"\\nSnow fraction by phase:\")\n",
    "snow_by_phase = imgs.groupby(\"phase\")[\"snow_frac\"].agg([\"mean\", \"std\", \"max\", \"count\"])\n",
    "print(snow_by_phase)\n",
    "\n",
    "# High snow events\n",
    "high_snow_events = change_df[\n",
    "    (change_df[\"pre_snow_frac\"] > 0.1) | (change_df[\"during_snow_frac\"] > 0.1)\n",
    "]\n",
    "if len(high_snow_events) > 0:\n",
    "    print(f\"\\nHigh snow events (snow_frac > 10%):\")\n",
    "    display(high_snow_events[[\"event_key\", \"change_pct\", \"pre_snow_frac\", \"during_snow_frac\"]])\n",
    "else:\n",
    "    print(\"\\nNo high snow events found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec11",
   "metadata": {},
   "source": [
    "# 11) Visualization: Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = (\n",
    "    imgs.groupby([\"event_key\", \"date\"])\n",
    "        .agg(\n",
    "            mean_ntl=(\"mean_ntl\", \"mean\"),\n",
    "            snow_frac=(\"snow_frac\", \"mean\"),\n",
    "            county=(\"county\", \"first\"),\n",
    "            event_id=(\"event_id\", \"first\"),\n",
    "            outage_start=(\"outage_start\", \"first\"),\n",
    "            outage_end=(\"outage_end\", \"first\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "        .sort_values([\"event_key\", \"date\"])\n",
    ")\n",
    "\n",
    "for event_key, g in ts.groupby(\"event_key\"):\n",
    "    g = g.sort_values(\"date\")\n",
    "    if g.empty:\n",
    "        continue\n",
    "    \n",
    "    start = g[\"outage_start\"].iloc[0]\n",
    "    end = g[\"outage_end\"].iloc[0]\n",
    "    county = g[\"county\"].iloc[0]\n",
    "    event_id = g[\"event_id\"].iloc[0]\n",
    "    max_snow = g[\"snow_frac\"].max()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # NTL line\n",
    "    color1 = \"tab:blue\"\n",
    "    ax1.plot(g[\"date\"], g[\"mean_ntl\"], marker=\"o\", linewidth=2, color=color1, label=\"Mean NTL\")\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Mean NTL (nW/cm2/sr)\", color=color1)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color1)\n",
    "    \n",
    "    # Snow fraction on secondary axis (if any snow)\n",
    "    if max_snow > 0.01:\n",
    "        ax2 = ax1.twinx()\n",
    "        color2 = \"tab:cyan\"\n",
    "        ax2.fill_between(g[\"date\"], 0, g[\"snow_frac\"], alpha=0.3, color=color2, label=\"Snow frac\")\n",
    "        ax2.set_ylabel(\"Snow Fraction\", color=color2)\n",
    "        ax2.tick_params(axis=\"y\", labelcolor=color2)\n",
    "        ax2.set_ylim(0, max(0.5, max_snow * 1.2))\n",
    "    \n",
    "    # Outage period\n",
    "    if pd.notna(start) and pd.notna(end):\n",
    "        ax1.axvspan(start, end, alpha=0.2, color=\"red\", label=\"Outage period\")\n",
    "\n",
    "    ax1.set_title(f\"{county} | {event_id}\\n{event_key}\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fname = f\"{event_key}_timeseries.png\"\n",
    "    plt.savefig(PLOT_DIR / fname, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "print(f\"Saved time series plots to: {PLOT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec12",
   "metadata": {},
   "source": [
    "# 12) POI Buffer Analysis Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "poi_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# POI data path\n",
    "POI_CSV = DATA / \"raw\" / \"POI\" / \"texas_critical_infra_points_2022.csv\"\n",
    "\n",
    "poi = pd.read_csv(POI_CSV)\n",
    "poi = poi.rename(columns={\"lon\": \"longitude\", \"lat\": \"latitude\"})\n",
    "\n",
    "gdf_poi = gpd.GeoDataFrame(\n",
    "    poi,\n",
    "    geometry=gpd.points_from_xy(poi[\"longitude\"], poi[\"latitude\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Project to metric CRS (Texas Centric Albers)\n",
    "gdf_poi = gdf_poi.to_crs(epsg=3083)\n",
    "\n",
    "# 1000m buffer\n",
    "BUFFER_M = 1000\n",
    "gdf_poi[\"geometry\"] = gdf_poi.geometry.buffer(BUFFER_M)\n",
    "\n",
    "# Map city_source to county name\n",
    "CITY_TO_COUNTY = {\n",
    "    \"Houston\": \"Harris\",\n",
    "    \"Dallas\": \"Dallas\",\n",
    "}\n",
    "gdf_poi[\"county\"] = gdf_poi[\"city_source\"].map(CITY_TO_COUNTY)\n",
    "gdf_poi = gdf_poi[gdf_poi[\"county\"].notna()].copy()\n",
    "\n",
    "print(f\"POI count: {len(gdf_poi)}\")\n",
    "print(f\"Counties: {gdf_poi['county'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec13",
   "metadata": {},
   "source": [
    "# 13) Calculate Buffer vs Non-buffer NTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buffer_calc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zonal_nanmean(src, geom):\n",
    "    \"\"\"Calculate nanmean within geometry\"\"\"\n",
    "    try:\n",
    "        out, _ = mask(src, [geom], crop=False, all_touched=True)\n",
    "        arr = out[0].astype(\"float32\")\n",
    "        if src.nodata is not None:\n",
    "            arr[arr == src.nodata] = np.nan\n",
    "        return float(np.nanmean(arr))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "# Create buffer union for each county\n",
    "buffers_by_county = {\n",
    "    c: gdf_poi[gdf_poi[\"county\"] == c].geometry.unary_union\n",
    "    for c in gdf_poi[\"county\"].unique()\n",
    "}\n",
    "\n",
    "print(f\"Buffer counties: {list(buffers_by_county.keys())}\")\n",
    "\n",
    "records = []\n",
    "error_count = 0\n",
    "\n",
    "for idx, r in imgs.iterrows():\n",
    "    county = r[\"county\"]\n",
    "    tif = r[\"path\"]\n",
    "\n",
    "    if county not in buffers_by_county:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with rasterio.open(tif) as src:\n",
    "            extent = box(*src.bounds)\n",
    "\n",
    "            # Project buffer to TIF's CRS\n",
    "            buf_geom = gpd.GeoSeries(\n",
    "                [buffers_by_county[county]], \n",
    "                crs=gdf_poi.crs\n",
    "            ).to_crs(src.crs).iloc[0]\n",
    "\n",
    "            # Non-facility area = extent - buffer\n",
    "            nonbuf_geom = extent.difference(buf_geom)\n",
    "\n",
    "            buf_mean = zonal_nanmean(src, buf_geom)\n",
    "            nonbuf_mean = zonal_nanmean(src, nonbuf_geom) if not nonbuf_geom.is_empty else np.nan\n",
    "\n",
    "        rec = dict(r)\n",
    "        rec[\"mean_ntl_buffer\"] = buf_mean\n",
    "        rec[\"mean_ntl_nonbuffer\"] = nonbuf_mean\n",
    "        records.append(rec)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {tif.name}: {e}\")\n",
    "        error_count += 1\n",
    "\n",
    "print(f\"\\nProcessed: {len(records)}, Errors: {error_count}\")\n",
    "\n",
    "df_buf = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec14",
   "metadata": {},
   "source": [
    "# 14) Buffer vs Non-buffer Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buffer_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_phase_buffer = (\n",
    "    df_buf.groupby([\"event_key\", \"phase\"])\n",
    "          .agg(\n",
    "              county=(\"county\", \"first\"),\n",
    "              buffer_min=(\"mean_ntl_buffer\", \"min\"),\n",
    "              buffer_mean=(\"mean_ntl_buffer\", \"mean\"),\n",
    "              nonbuffer_min=(\"mean_ntl_nonbuffer\", \"min\"),\n",
    "              nonbuffer_mean=(\"mean_ntl_nonbuffer\", \"mean\"),\n",
    "              n_images=(\"mean_ntl_buffer\", \"count\"),\n",
    "              avg_snow_frac=(\"snow_frac\", \"mean\"),\n",
    "              outage_start=(\"outage_start\", \"first\"),\n",
    "              outage_end=(\"outage_end\", \"first\"),\n",
    "          )\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "event_phase_buffer.to_csv(RESULT / \"event_phase_buffer_vs_nonbuffer.csv\", index=False)\n",
    "print(\"Event x Phase x Buffer stats:\")\n",
    "display(event_phase_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec15",
   "metadata": {},
   "source": [
    "# 15) Buffer vs Non-buffer Change Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buffer_change",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Buffer vs Non-buffer Relative Change (during vs pre)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "buf_results = []\n",
    "for evt in event_phase_buffer[\"event_key\"].unique():\n",
    "    evt_df = event_phase_buffer[event_phase_buffer[\"event_key\"] == evt]\n",
    "    \n",
    "    pre = evt_df[evt_df[\"phase\"] == \"pre\"]\n",
    "    during = evt_df[evt_df[\"phase\"] == \"during\"]\n",
    "    \n",
    "    if len(pre) > 0 and len(during) > 0:\n",
    "        pre_buf = pre[\"buffer_mean\"].values[0]\n",
    "        during_buf = during[\"buffer_mean\"].values[0]\n",
    "        pre_nonbuf = pre[\"nonbuffer_mean\"].values[0]\n",
    "        during_nonbuf = during[\"nonbuffer_mean\"].values[0]\n",
    "        \n",
    "        buf_change = ((during_buf - pre_buf) / pre_buf) * 100 if pre_buf > 0 else np.nan\n",
    "        nonbuf_change = ((during_nonbuf - pre_nonbuf) / pre_nonbuf) * 100 if pre_nonbuf > 0 else np.nan\n",
    "        \n",
    "        buf_results.append({\n",
    "            \"event_key\": evt,\n",
    "            \"county\": pre[\"county\"].values[0],\n",
    "            \"buf_change_pct\": round(buf_change, 1) if not np.isnan(buf_change) else np.nan,\n",
    "            \"nonbuf_change_pct\": round(nonbuf_change, 1) if not np.isnan(nonbuf_change) else np.nan,\n",
    "            \"diff\": round(buf_change - nonbuf_change, 1) if not (np.isnan(buf_change) or np.isnan(nonbuf_change)) else np.nan,\n",
    "            \"pre_snow\": round(pre[\"avg_snow_frac\"].values[0], 3),\n",
    "            \"during_snow\": round(during[\"avg_snow_frac\"].values[0], 3),\n",
    "        })\n",
    "\n",
    "buf_change_df = pd.DataFrame(buf_results)\n",
    "print(\"\\nBuffer vs Non-buffer change:\")\n",
    "display(buf_change_df)\n",
    "\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  diff > 0: Buffer dropped LESS than non-buffer (facilities more resilient)\")\n",
    "print(f\"  diff < 0: Buffer dropped MORE than non-buffer\")\n",
    "print(f\"\\nEvents where buffer more resilient: {(buf_change_df['diff'] > 0).sum()} / {len(buf_change_df)}\")\n",
    "\n",
    "buf_change_df.to_csv(RESULT / \"buffer_change_analysis.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sec16",
   "metadata": {},
   "source": [
    "# 16) Buffer vs Non-buffer Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buffer_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_DIR_BUF = RESULT / \"plots_buffer\"\n",
    "PLOT_DIR_BUF.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ts_buf = (\n",
    "    df_buf.groupby([\"event_key\", \"date\"])\n",
    "          .agg(\n",
    "              buffer=(\"mean_ntl_buffer\", \"mean\"),\n",
    "              nonbuffer=(\"mean_ntl_nonbuffer\", \"mean\"),\n",
    "              snow_frac=(\"snow_frac\", \"mean\"),\n",
    "              county=(\"county\", \"first\"),\n",
    "              event_id=(\"event_id\", \"first\"),\n",
    "              outage_start=(\"outage_start\", \"first\"),\n",
    "              outage_end=(\"outage_end\", \"first\"),\n",
    "          )\n",
    "          .reset_index()\n",
    "          .sort_values([\"event_key\", \"date\"])\n",
    ")\n",
    "\n",
    "plot_count = 0\n",
    "for event_key, g in ts_buf.groupby(\"event_key\"):\n",
    "    g = g.sort_values(\"date\")\n",
    "    if g.empty or g[\"buffer\"].isna().all():\n",
    "        continue\n",
    "\n",
    "    start = g[\"outage_start\"].iloc[0]\n",
    "    end = g[\"outage_end\"].iloc[0]\n",
    "    county = g[\"county\"].iloc[0]\n",
    "    event_id = g[\"event_id\"].iloc[0]\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    ax1.plot(g[\"date\"], g[\"buffer\"], marker=\"o\", linewidth=2, \n",
    "             color=\"tab:orange\", label=\"Critical facility buffer (1km)\")\n",
    "    ax1.plot(g[\"date\"], g[\"nonbuffer\"], marker=\"s\", linewidth=2, \n",
    "             color=\"tab:blue\", label=\"Non-facility area\")\n",
    "    \n",
    "    if pd.notna(start) and pd.notna(end):\n",
    "        ax1.axvspan(start, end, alpha=0.2, color=\"red\", label=\"Outage period\")\n",
    "\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.set_ylabel(\"Mean NTL (nW/cm2/sr)\")\n",
    "    ax1.set_title(f\"{county} | {event_id}\\n{event_key}\\nBuffer vs Non-buffer\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    fname = f\"{event_key}_buffer_vs_nonbuffer.png\"\n",
    "    plt.savefig(PLOT_DIR_BUF / fname, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    plot_count += 1\n",
    "\n",
    "print(f\"Saved {plot_count} buffer plots to: {PLOT_DIR_BUF}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}